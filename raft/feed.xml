<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>灵元</title>
		<description>给时光以生命，给存在以觉知</description>
		<link>http://lihui.github.io</link>
		<atom:link href="http://lihui.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Kafka:一种支持日志处理的分布式消息系统（译）</title>
				<description>&lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;    日志处理正成为消费型互联网公司的数据处理管道中的关键组件，我们在此介绍Kafka，这是一个我们开发的分布式消息系统，能低延时地收集和分发海量日志数据。我们的系统结合了来自于已有的日志聚合器和消息系统的思想，同时适合在线和离线的消息消费。我们采取了非同寻常然但确有实效的设计决策使得系统高效并具备可伸缩性。我们的实验结果显示，与两款流行的消息系统相比，Kafka具有超凡性能。我们已经在生产环境中使用Kafka有一段时间了，它每天处理着数百GB的新数据。&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;1-引言&quot;&gt;1 引言&lt;/h2&gt;

&lt;p&gt;在任何规模的互联网公司中都会产生大量的日志数据。这些数据典型地包括（1）用户活动事件，如登录，页面查看，点击，点击“like”（+1），分享，评论以及查询搜索；（2）操作指标，如服务调用栈，调用延时，错误以及系统性能指标如每台机器上的CPU，内存，网络，磁盘利用率。日志数据作为分析组件用来跟踪用户、系统利用率和其他指标已经有很长历史了。然而最近互联网应用的趋势是将活动数据作为生产数据处理管道的一部分直接用于网页中的某些功能特性。这些使用场景包括（1）搜索相关排序（2）推荐，通过项目（商品）流行度和活动流中的共现性来实现（3）广告定向和报告（4）安全，避免滥用行为如垃圾邮件和未经授权的数据抓取（5）新闻聚合，聚合用户状态更新或者动作，让此用户的朋友或者连接器能够订阅。&lt;/p&gt;

&lt;p&gt;这种线上实时日志数据的使用对数据系统提出了新的挑战，因为它的数据规模相对于“真实”的业务数据有数量级的提升。比如，搜索，推荐，和广告通常要求单个计算点击率，这样产生的日志不但要记录对每一个用户点击项，还要把每一个页面中几十个未被点击的项也记录下来。中国移动收集了5-8TB的手机拨打记录，facebook也收集了近乎6TB的不同用户的活动事件。&lt;/p&gt;

&lt;p&gt;许多早期的处理类似数据的系统依赖于物理抓取生产服务器的日志文件进行分析。在最近几年，出现了几种专门的分布式日志聚合器，包括Facebook的 Scribe，Yahoo的Data Highway，和Cloudera的Flume。这些系统主要被设计为收集和载入日志数据到数据仓库或者Hadoop中以便离线使用。在LinkedIn（一种社交网络站点），我们发现除了传统的离线分析，还需要支持绝大多数如上所述的实时应用，要求延迟不超过几秒钟。&lt;/p&gt;

&lt;p&gt;我们为日志处理构造了一个名为Kafka的新的消息系统，该系统结合了传统日志聚合器和消息系统的优点。在一方面，Kafka是一个分布式的可伸缩的系统，提供很高的吞吐量。在另外一方面，Kafka提供了一个类似于消息系统的API，允许应用实时地消费日志事件。Kafka已经开源并且成功地运用于LinkedIn的生产系统超过了6个月。它极大地简化了我们的基础设施。因为我们用一个软件可统一为各种类型的日志数据进行在线和离线处理。本论文组织如下：我们在第二章回顾传统的消息系统和日志聚合器；第三章描述Kafka的架构机器关键设计原则；第四章描述Kafka在LinkedIn的部署；第五章给出性能结果；最后在第六章讨论未来的工作以及给出总结。&lt;/p&gt;

&lt;h2 id=&quot;2-相关工作&quot;&gt;2 相关工作&lt;/h2&gt;

&lt;p&gt;传统企业消息系统已经存在了很长一段时间了，它们作为事件总线为异步数据流处理起了关键作用。然而，有一些原因使得它们不适合日志处理。首先，日志处理和企业消息系统在需求特性上不匹配。他们往往专注于提供一套丰富的消息传送保证机制。比如，IBM Websphere MQ提供了事务支持，允许应用原子地插入消息到多个队列中。JMS规范允许每一个消息在消费后都进行确认，并且可以是无序的。这些传送保证机制对日志收集而言通常是没有必要的。例如，偶尔丢失了一些页面查看事件并没有什么大不了的。这些非必要的特性增加了API和系统底层实现的复杂度。其次，很多系统并不十分关注于吞吐量并以此为主要设计约束。比如JMS没有API使得生产者可以显式地将消息批量打包到一个请求中去。这意味着每一个消息传送都会导致一个完整的TCP/IP来回，这种方式不能满足我们这个领域中的吞吐量的需要。第三，这些系统在分布式支持上比较弱。在多台机器上进行消息分区和存储并不容易。最后，许多消息系统假设消息被立即（或者接近立即）消费，所以未被消费的消息队列总是相当小的。如果消息允许被归集起来，那么它们的性能就会有严重的下降，这种场景在离线消息消费中会出现，比如数据仓库应用是定期执行大量数据导入，而不是连续消费消息。&lt;/p&gt;

&lt;p&gt;在过去的几年里，出现了一些专门的日志聚合器。Facebook使用一个叫Scribe的系统，每一个前端机器可以通过socket发送日志数据到一个Scribe机器集，每一个Scibe聚合日志条目并且定期导入到HDFS或者NFS设备。Yahoo的data highway项目有着类似的流程。一群机器从客户端集合事件然后滚动到“minute”文件，这些文件再被加入到HDFS。flume是一个由Cloudera开发的相对新的日志聚合器，它支持可扩展的“pipes”和“sinks”，使日志流处理过程非常灵活，它还有更紧密的分布式支持。然而绝大多数这些系统是为消费离线日志数据而设计的，并且常常不必要地暴露了实现细节给消费者（例如：“minute”文件）。另外，它们大多数使用“push”模型，broker（译注：代理，指消息系统）转发消息给消费者。在LinkIn，我们发现“pull”模型更适合我们的应用，因为这样每一个消费者能以它能支持的最大的速率获取消息，避免了它因处理能力不足而产生的消息洪泛现象。“pull”模型也使得消费者能够容易地回退，以重新读取以前某个时间的消息，我们将在第2.3节讨论这种功能的好处。&lt;/p&gt;

&lt;p&gt;最近，Yahoo! 研究院开发了一个新型的名为HedWig的分布式pub/sub系统。HedWig具有高度可伸缩性和可用性，并且提供了强持久化保证。然而，他主要目的是用来存储数据仓库的提交日志（commit log)。&lt;/p&gt;

&lt;h2 id=&quot;3-kafka架构和设计原则&quot;&gt;3 Kafka架构和设计原则&lt;/h2&gt;

&lt;p&gt;因为现有系统的限制，我们开发了一个新的基于消息的日志聚合系统Kafka。我们首先介绍Kafka中的基本概念。一种特殊类型的消息流定义为topic，生产者可以发布消息到一个topic。发布的消息然后存储在一群称为broker的服务器上。一个消费者可以订阅一个或者多个topic，并且通过拉的方式从broker消费数据。&lt;/p&gt;

&lt;p&gt;在概念上消息很简单，我们也努力使得Kafka API也同样简单。我们将不给出API细节，而是通过一些代码样例来展示API是如何使用的。生产者的样例代码在下面给出。一个消息仅仅定义为包含字节的载荷，用户可以选择他所喜好得序列化方法来编码消息。为了效率，生产者可以在一个发布请求中发送一组消息。
&lt;pre&gt;&lt;code&gt;Sample producer code：
producer=new Producer(...);
message=new Message(&amp;quot;test message str&amp;quot;.getBytes());
set=new MessageSet(message);
producer.send(&amp;quot;topic1&amp;quot;,set);&lt;/code&gt;&lt;/pre&gt;
为了订阅一个topic，消费者首先为这个topic创建一个或者多个消息流。消息发布到那个topic最终将分发到这些子流中。关于Kafka分发消息得细节将在3.2节叙述。每一个消息流在产生的连续流上提供了一个迭代器接口。然后消费者迭代流中的每一个消息并且处理消息中得载荷。与传统的迭代器不同，消息流迭代器从不结束。如果当前没有更多的消息可供消费，迭代器将会阻塞知道新消息发布到该topic上。我们既支持point-to-point传送模型，也支持publish/subscribe模型。point-to-point传送模型中多个消费者一起消费topic中消息的一个拷贝（比如一个消息随机分发给其中一个消费者）。publish/subscribe模型中每个消费者能够获得自己的一个独立的拷贝。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Sample consumer code:
streams[]=Consumer.createMessageStream(&quot;topic1&quot;,1);
for(message:stream[0]){
    bytes=message.payload();
    //do something with the bytes
}&lt;/code&gt;&lt;/pre&gt;
    

&lt;p&gt;Kafka的整体架构展示在图1中。由于Kafka天然是分布式的，kafka集群典型地由多个broker组成。为了负载均衡，一个topic分割成多个分区，每一个broker存储一个或者多个这样的分区。多个生产者和消费者可以在任意时间发布和读取消息。在3.1节我们将描述单个分区在broker上的布局，以及一些为了高效访问分区而采纳的设计决策。在3.2节我们将描述生产者和消费者在分布式环境下与多个broker如何交互的。在3.3节我们讨论Kafka的传送保证机制。
&lt;div align=&quot;center&quot;&gt;&lt;img style=&quot;width:80%&quot; src=&quot;/media/kafka-arch.png&quot;&gt;&lt;/img&gt;&lt;/div&gt;&lt;/p&gt;

&lt;h3 id=&quot;3.1-单分区中的效率&quot;&gt;3.1 单分区中的效率&lt;/h3&gt;

&lt;p&gt;我们在Kafka中使用了一些决策来提高系统性能。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;简单存储&lt;/strong&gt;：Kafka有一个非常简单得存储布局。每个topic的分区对于一个逻辑日志。物理地，一个日志实现为一组大小近似的分段文件（如1GB）。每一次生产者发布一个消息到一个分区的时候，broker简单地讲这个消息追加到最后一个分段文件里。为了获得更好的性能，我们仅仅在经过一定数目的消息之后或者一段时间之后才将分段文件flush（刷新）到磁盘中，这些数目和时间是可配置的。一个消息仅当它flush到磁盘后才能暴露给消费者。&lt;/p&gt;

&lt;p&gt;与典型的消息系统不同，一个消息存储在Kafka中并没有一个显式的消息id。相反，每一个消息通过它在日志中的逻辑偏置(logical offset)来寻址。这种方法避免了因维护多余的、寻道密集的、随机访问的索引结构而产生的开销，否则我们需要这种索引结构将消息id映射到消息在日志文件中的位置。需要注意的是我们的消息id是增量的但并不连续。为了计算下一个消息id，我们必须将当前消息id加上当前消息的长度。从现在开始，我们将等价使用消息id和偏置。&lt;/p&gt;

&lt;p&gt;一个消费者从特定的分区中总是顺序地消费信息。如果消费者确认一个特定的消息偏置，这将隐含表明该消费者已经收到了此offset以前的在本分区中的所有消息。在底层实现中，消费者发送一个异步pull请求到broker获取一个此应用要消费的准备就绪的数据。每一个pull请求包含了要消费的起始消息的offset和能够接受的字节大小。每个broker在内存中持有一个offset的有序列表，该列表中的offset是每个分段文件中第一个消息的offset（逻辑偏置）。broker通过搜索有序列表来定位所请求的消息在那个分段文件中，然后将数据发送会消费者。消费者接受到消息后，它计算下一个要消费的消息的offset，将其用在下一个pull请求中。Kafka的日志布局和内存中的索引展示在图2中。每个框内显示了消息的offset。
&lt;div align=&quot;center&quot;&gt;&lt;img style=&quot;width:80%&quot; src=&quot;/media/kafka-segmentfile.png&quot;&gt;&lt;/img&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;高效传输&lt;/strong&gt;: 我们非常关心Kafka中数据的传进传出。前文我们提到生产者可以在一个发送请求中提交一组消息。尽管终端消费API一次迭代一个消息，但在底层实现中，每一个pull请求也会读取多个消息直到抵达了某个字节数量的上限，典型地，这个数值是几百个KB。&lt;/p&gt;

&lt;p&gt;另外一个非同寻常的抉择是我们避免显式地在kafka中缓存消息。相反，我们依赖于底层的文件系统页缓存机制(page cache)。这种方式的主要好处是可以避免双重缓存——消息仅仅缓存在页缓存中。另外一个好处是即使broker进程重启了，缓存还能保持“热度”。既然Kafka完全不在进程中缓存消息，它在内存垃圾回收上开销就很小，这使得基于虚拟机的语言也能够高效实现这个系统。最后，因为生产者和消费者都顺序地访问分段文件，消费者的访问常常紧接着生产者，正常的启发式操作系统缓存机制将会非常有效(特别是write-through和read-ahead缓存)。我们发现生产者和消费者在数据集大到若干TB的时候，都一致地具有线性性能。&lt;/p&gt;

&lt;p&gt;另外我们还为消费者优化了网络访问性能。Kafka是一个多订阅的系统，一个消息可以被不同的消费者应用消费多次。从本地文件发送字节到远程socket一个典型的方案包含如下步骤（1）从存储媒介上读取数据到OS的页缓存中（2）从页缓存拷贝数据到应用缓冲区(buffer)(3)从应用缓冲区拷贝到另一个内核缓冲区（4）从内核缓冲区发送到socket。这包含4个数据拷贝和2个系统调用。在Linux和其他Unix操作系统中，存在一个sendfile API可以直接将字节从文件通道传送到socket通道。这典型地避免了第（2)(3)步中的2个拷贝和1个系统调用。Kafka利用sendfile API高效地将字节数据从日志分段文件中传输到消费者。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;无状态broker&lt;/strong&gt;: 与大多数消息系统不同，在Kafka中关于每个消费者消费了多少的信息并不保存在broker中，而是由消费者自行保存。这种设计为broker降低了很多开销和系统复杂性。然而，这使得删除消息变得微妙起来，因为broker不知道是否所有的订阅者都消费了这个消息。Kafka通过使用一个简单的基于时间的SLA保留策略来解决这个问题。如果在broker中保留超过一段时间（如7天），一个消息将被自动删除。这个方案在实践中工作良好。因为包括离线消费者在内的大多数消费者，都将在一天、一个小时乃至于实时完成消息消费。又由于Kafka的性能在大数据集上并不会降低，这个事实允许我们进一步延长消息保留的时间，从而更可靠地解决了消息删除的问题。&lt;/p&gt;

&lt;p&gt;这种设计（指无状态）有一个额外的重要好处，一个消费者可以自由地回退到老的消息offset从而可以重新消费数据。这违反了关于队列的一般契约，但被证明了对于很多消费者而言是一个基本的特性需要。例如，当消费者应用逻辑中有一个错误时，在错误修复后，应用可以重放某些消息。这在ETL工具将数据载入到数据仓库或者hadoop系统中时特别重要。再如另外一个例子，消费过的数据可能仅定期flush到持久存储中（如，全文索引），如果消费者崩溃了，那么未flush的数据就会丢失。在这种情况下，当它重启后，消费者可以检查未flush的消息的最小的offset，然后从该offset开始重新消费消息。值得指出的是，在pull模型中支持消费者回退要比push模型中容易得多。&lt;/p&gt;

&lt;h3 id=&quot;3.2-分布式协调&quot;&gt;3.2 分布式协调&lt;/h3&gt;

&lt;p&gt;现在我们描述生产者和消费者在分布式环境下如何工作。每一个生产者发送消息到一个分区中，这个分区或者是随机选择的，或者是用分区函数和分区键值所确定的。而下面我们将关注消费者如何与broker进行交互。&lt;/p&gt;

&lt;p&gt;Kafka有一个叫&amp;quot;consumer groups&amp;quot;的东西，每个consumer group由一个或者多个消费者组成，它们一起消费所订阅的主题(topic)集合。也就是说，每一个消息仅仅送达到这个consumer group中的一个消费者。不同的consumer group独立地消费所有的所订阅的消息，与其他的consumer group完全不相干。consumer group中的消费者可能在不同的进程或者不同的机器上运行。我们的目标是将存储在各broker中的消息均匀地分发给消费者，而无需引入太多的协调开销。&lt;/p&gt;

&lt;p&gt;我们第一个抉择是使得一个主题的分区是最小的并行单元。这意味着在任何时候，一个分区的全部信息只被一个每个consumer group的一个消费者所消费。我们曾允许多个消费者并发地消费一个分区，他们因而必须就谁消费了那条信息这样的问题进行协调，这就必须引入锁和状态管理的开销。相反，现今我们的设计中消费者进程仅需在执行负载均衡时进行协调，而这个操作并不经常发生。为了让负载真正均衡，我们要求topic中的分区数量要远大于consumer group中的消费者的数目。这我们可以通过简单地对topic划分很多分区而实现。&lt;/p&gt;

&lt;p&gt;我们所作的第二个抉择是不设置中心主节点，而以去中心化的方式让消费者自己进行协调。添加一个主节点会导致系统复杂，因为我们还得关心主节点失效的问题。为了进行协调，我们采用了高可用的一致性服务Zookeeper。Zookeeper，有一个非常简单的类文件系统的API。藉此可以创建路径，设置路径的值，读取路径的值，删除路径，列出路径下的子路径等。它还可以做一些更有趣的事情，例如：(a)可以在路径上注册观察器，当子路径或者该路径上的值发生改变的时候获得通知；(b)一个路径可以创建为临时的(与持久路径相反),这意味着如果创建这个路径的客户端离线了，该路径就会被自动删除；(c)Zookeeper将其数据复制在多台服务器上，这使得数据具备高可靠性和高可用性。&lt;/p&gt;

&lt;p&gt;Kafka使用Zookeeper完成如下任务：(1)检测broker和consumer的加入和移除，(2)当上述事件发生时，触发负载均衡的过程，以及(3)维护消费关系并且跟踪每个分区所消费过的offset。特别地，当每一个broker和consumer启动时，它将其信息注册到Zookeeper。broker注册表包括broker的主机名、端口号，以及其所存储的topic和分区信息。consumer注册表包含其所属于的consumer group，以及它所订阅的topic集合。每个consumer group的从属注册表（描述consumer和consumer group从属关系）和offset注册表息也存储在Zookeeper中。每个订阅的分区在从属注册表中均有一条路径，路径上的值是当前正在消费该分区的consumer 标示符。offset注册表存有每一条订阅分区的最近被消费的消息位置偏移（offset）。&lt;/p&gt;

&lt;p&gt;broker注册表、consumer注册表、从属注册表、offset注册表在Zookeeper中所创建的诸路径是易失的。如果一个broker失效了，在其之上的所有分区信息都自动从broker注册表中移除。一个consumer的失效将导致它在consumer注册表中的条目，以及从属注册表中它所拥有的所有分区都被删除。每个consumer在broker注册表和consumer注册表都注册了一个Zookeeper观察器，一旦任何broker或者consumer group有相关的变更，它都将得到通知。&lt;/p&gt;

&lt;p&gt;在consumer启动时，或者当consumer得到一个关于broker/consumer变更的通知，consumer将会发起一个负载重平衡过程，以确定它所应消费那些新分区。该过程在算法1中描述。
&lt;pre&gt;&lt;code&gt;算法1：负载均衡过程
输入：consumer $C_i$ ，consumer group $G$
[foreach]: 对$C_i$所订阅的每一个topic $T${
    从从属注册表中删除$C_i$所拥有的分区
    从Zookeeper中读取broker、consumer注册表
    计算$P_T$=在topic $T$下所有broker的分区总集
    计算$C_T$=G中所有订阅topic $T$的consumer
    对$P_T$,$C_T$排序
    设$j$是$C_i$在$C_T$中的索引,且$N=\frac{|P_T|}{|C_T|}$
    将序号从$j*N$到$(j+1)*N-1$的分区分配给consumer $C_i$
    [foreach]: 对每一个被分配的分区$p$ {
        在从属注册表中将$p$的拥有者设置为$C_i$
        设$O_p$=分区$p$存储在offset注册表中的offset
        启用一个线程从offset(偏移)$O_p$开始在分区$p$中拉取数据
    }
}&lt;/code&gt;&lt;/pre&gt;
通过从Zookeeper中读取broker注册表和consumer注册表，consumer首先计算为每个订阅主题$T$计算其分区集$(P_T)$，以及$G$中订阅主题$T$的consumer集合$C_T$。然后根据范围将分区集合$P_T$切分为段，将每一段分别分配给$C_T$中的consumer。对于consumer所分配到的每一个分区，consumer都在从属注册表中把自己记录为此分区的新拥有者。最后，consumer开启一个线程从每一个所拥有的分区中拉取数据，起始位置是记录在offset注册表中的偏移（offset）。当消息从一个分区中拉取出来后，consumer在offset注册表中定期更新上次消费过的消息的offset。&lt;/p&gt;

&lt;p&gt;当某个consumer group中有多个consumer时，broker或者consumer发生变动时每一个consumer都会得到通知。然而，各consumer得到通知的时间可能会有些许差异。所以一个consumer有可能会试图获取另外一个consumer所拥有的分区。当此情况发生时，第一个consumer将直接释放他所拥有的所有分区，等待一会儿然后重试负载均衡过程。在实际中，负载均衡过程通常会在几次尝试后就稳定下来。&lt;/p&gt;

&lt;p&gt;当一个新consumer group创建时，offset注册表中没有offset信息。在这种情况下，通过使用我们所提供的broker的API，consumer可从所订阅分区的最小或者最大的offset开始获取消息，具体依配置而定。&lt;/p&gt;

&lt;h3 id=&quot;3.3-传送保证&quot;&gt;3.3 传送保证&lt;/h3&gt;

&lt;p&gt;通常，Kafka仅保证“至少一次”的消息传送。“确保一次”的消息传送一般要求两阶段提交协议，这在我们的应用中不是必须的。绝大多数时候，消息能够“确保一次”传送给每个consumer group。然而当consumer 进程崩溃没有正常关闭时，接管失效consumer所拥有的那些分区的consumer进程可能会获取到一些重复的消息，这些消息是上次失效consumer成功提交到ZooKeeper的offset之后的，处理过但未及提交的那些消息。如果应用对这种重复不能接受，那么它必须自行添加重复删除逻辑，这可以使用consumer接收到的offset信息，或者消息中的唯一键值来实现。这种方案通常比两阶段提交协议更加经济有效。&lt;/p&gt;

&lt;p&gt;Kafka保证单个分区内的消息传送的有序性。然而来自于不同分区的消息之间不能保证先后顺序。&lt;/p&gt;

&lt;p&gt;为了避免消息损坏，Kafka在日志中为每条消息存储一个CRC校验码。如果在broker中又任何I/O错误，Kafka运行一个恢复进程删除与CRC校验码不一致的消息。在消息级加入CRC也使得我们可以在消息产生和消费后检测网络错误。&lt;/p&gt;

&lt;p&gt;如果某个broker下线了，任何存储在其上的还未被消费的消息都将无法访问。如果某broker上的存储系统永久损坏了，任何未被消费的消息都会永远丢失。在未来（译注：最新版本已经实现）我们计划在Kafka中添加内置的备份机制，为每条消息在多个broker上提供存储冗余。&lt;/p&gt;

&lt;h2 id=&quot;4-kafka在linkedin的应用&quot;&gt;4 Kafka在LinkedIn的应用&lt;/h2&gt;

&lt;p&gt;在本节中，我们描述Kafka是如何在LinkedIn使用的。图3展示了我们部署方案得一个简化版本。我们为每一个运行面向用户的服务的数据中心配备一个Kafka集群。前端服务产生多种日志数据，批量发布到本地Kafka broker。我们依赖硬件负载均衡将发布请求匀布给该组Kafka broker。Kafka的在线consumer也运行在同一个数据中心。
&lt;div align=&quot;center&quot;&gt;&lt;img style=&quot;width:80%&quot; src=&quot;/media/kafka-deployment.png&quot;&gt;&lt;/img&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;为了离线分析我们也在一个独立的数据中心部署了一个Kafka集群，在地理位置上靠近我们的hadoop集群和其他得数据仓库设施。这个kafka实例运行了一组内嵌的consumer从线上数据中心拉取数据。我们然后运行数据加载作业从该备份Kafka集群拉取数据到hadoop和我们的数据仓库，在那里我们对此数据运行多种报表作业和分析流程。我们还使用该Kafka集群做原型实验，可以在原始事件流上运行简单的脚本执行adhoc查询。无需过多的调优，全部处理管道的端对端延迟平均大约10秒，很好地满足了我们的需求。&lt;/p&gt;

&lt;p&gt;当前Kafka每天收集数百G的数据，接近十亿条消息，当我们完成了遗留系统得迁移我们预期它还将显著增加。未来将会加入更多种类的消息。当运营团队进行软件或者硬件维护时，会启动或者停止broker，此时负载均衡过程可以自动重定向消息消费。&lt;/p&gt;

&lt;p&gt;我们的跟踪系统也包含一个审核组件，验证整个处理管道中的没有发生数据丢失。为了支持该功能，每条消息在产生时包含了时间戳和服务器名。我们要求每个producer定期产生一个监控事件，记录此producer在某固定时间窗口中对每个topic发送了多少条消息。producer以一个独立的topic发布监控事件。consumer可统计它所接受到的某topic的消息数量，与监控事件中的计数相比对就能验证数据的正确性。&lt;/p&gt;

&lt;p&gt;通过实现一个特殊的Kafka input format，我们可将数据加到hadoop集群。这允许MapReduce作业直接从Kafka中读取数据。一个MapReduce作业加载原始数据然后进行分组和压缩，使得未来可进行高效处理。broker的无状态性和客户端存储消息offset的方案在此又一次显示出了优势，这使得MapReduce任务管理（允许任务失效和重启）能够以自然的方式处理数据加载，当任务重启时不会产生数据重复和丢失的情况。仅当作业成功完成时，数据和offset才会一起被存储在HDFS中。&lt;/p&gt;

&lt;p&gt;我们选择使用Avro作为我们的序列化协议，因为它高效且支持schema演化。对每条消息，我们存储它的Avro schema的id和序列化后的字节载荷。我们可以藉此schema施加契约确保producer和consumer之间的数据兼容性。我们使用一个轻量级的schema注册服务将schema id映射到实际的schema。当consumer获取一条信息时，它在schema注册表中查找并读取schema，然后使用该schema将字节数据解码成对象（对每个schema查找过程只需要执行一次，因为schema是不可变的值）。&lt;/p&gt;

&lt;h2 id=&quot;5、6-（从略）&quot;&gt;5、6 （从略）&lt;/h2&gt;
</description>
				<pubDate>Wed, 08 Jul 2015 00:00:00 +0800</pubDate>
				<link>http://lihui.github.io/2015/07/kafka</link>
				<guid isPermaLink="true">http://lihui.github.io/2015/07/kafka</guid>
			</item>
		
			<item>
				<title>Logistic回归的特征工程</title>
				<description>&lt;h2 id=&quot;1-函数的表示&quot;&gt;1 函数的表示&lt;/h2&gt;

&lt;h3 id=&quot;1.1-人类使用的函数&quot;&gt;1.1 人类使用的函数&lt;/h3&gt;

&lt;p&gt;从中学开始，我们所学的大多是连续函数，有些函数特别漂亮，比如抛物线，正弦函数。这些函数似乎以一个简单的公式，就描述了无限的x与y的对应关系。这些代数函数和超越函数，都是基于代数运算规则来定义的。其中代数函数由有限个代数函数构成，超越函数则通过无限次迭代以代数函数为基础合成，在这个迭代过程中，仅有有限个参数或者没有参数来控制迭代规则。例如指数函数的泰勒展开式就定义了这样的迭代规则。用这些函数来表达映射关系的时候，对于任意的定义域和值域，均只需要有限的参数就可以确定映射的唯一性。这是一种非常优秀的特性，能通过抓住极少数变化的要素（参数），就可以确定描述大量事实之间的依存关系。当找到了一个这样的函数我们一般认为找到了一个简洁的规律。通过这个规律能够推演出无限的事实来，在自然科学研究中叫规律的普适性。在机器学习中，称为泛化能力。&lt;/p&gt;

&lt;h3 id=&quot;1.2-机器使用的函数&quot;&gt;1.2 机器使用的函数&lt;/h3&gt;

&lt;p&gt;机器学习要寻找这种具备很好的泛化能力的模型。但计算机更加擅长处理数值计算问题，对符号（公式、计算规则）很难处理。由于计算公式的多样性与可组合性，机器不可能通过穷尽所有的公式所构成的函数空间，更由于某些函数可能没有闭形式，因此机器学习所学习到的函数一般是限定在某个函数空间下，通过搜索其参数的恰当值而得到的。&lt;/p&gt;

&lt;p&gt;这样看起来似乎计算机的处理能力很受限，然而我们可以通过使用分段常量或者分段线性函数来逼近任意足够光滑的函数。如果用一个参数来定义一个分段常量函数，或者两个参数来定义分段线性函数。同时将该参数数量控制在一个足够大的有限量上。通过这种方式计算机在理论上能够建模实际所需的任意函数。当然我们要避免让计算机直接构造每一对映射，定义域一般是巨大的，无穷的，甚至是不可数的。如果只记忆部分样本，无法对不在记忆中的实例进行推断，因此不具备推广（泛化）能力。分段常量或者分段线性函数提供了一种近似预测相邻未知点的值的方法。通过使用比原数据更少的模型参数来表达原始海量的值对。只要模型参数的数量比原数据更少，就能够获得某种程度的泛化能力。实际上能够使用越少的参数（或者同样的参数数目下参数越稀疏）拟合数据就越可能找到了“规律”。&lt;/p&gt;

&lt;h4 id=&quot;1.2.1-分类常量&quot;&gt;1.2.1 分类常量&lt;/h4&gt;

&lt;p&gt;作为一个例子，我们来考虑如何以有限的参数，以分段常量函数来近似具有无限个值对的正弦函数。首先，我们能够以足够多的有限个分段常量函数获得所需足够精度的一个周期的近似，假设该数目为k，周期为T。其次，我们可以针对每一个x值构造k个特征，若x处于区间 [nT+iT/k,nT+(i+1)T/k),则令x的第i个特征为1，其余特征为0. 如果x满足满足第i个特征，我们便给出其值为第i个常量函数的值。这样，我们便通过使用一种固定特征提取的方式，和有限个分段常量函数（有限个参数），建模了正弦函数。这个模型具有很好的泛化能力。其精度随着参数数目的增加而增加。当特征数目确定的时候，可以使用损失函数“误差平方和”来训练模型。&lt;/p&gt;

&lt;p&gt;由上可知，如何找到特征函数是问题的关键，而参数数量的大小在这里对应着特征的多少，决定了模型的精确度。当然由于正弦函数是周期函数，容易理解可以用有限个分段常量函数来表达，对于有些函数，比如二次函数，如果值域是无穷的，那么便无法通过有限个分段常量函数来近似。然而在实际问题中，我们所面临的情况，都是定义域（自变量）非常巨大，以至于在训练数据中不可能全部出现，但需要预测的值往往不是无穷区间，而是有着一个确定的区间，比如对于典型的二分类问题，只有两个值。对于这类问题，虽然不像正弦函数一样都具有良好的周期性，但如同拟合正弦函数一样，如果找到一些方法，将输入实例进行有效折叠（分类，等价于构造有效特征），那么仍然可以用有限的参数来建模函数。&lt;/p&gt;

&lt;p&gt;换个角度来看，每构造一个特征，其实都是给出了一个分类方法。正弦函数构造特征的过程，就是将属于某个分段，以及所有与该分段跨度为T的数均划分为一类，将其对应于某个常量，从这个意义上来说，上述&lt;strong&gt;分段常量&lt;/strong&gt;应该称为&lt;strong&gt;分类常量&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;分类常量作为万能建模法，对机器学习在能力上给出了一个保障。若论原初，这个保障来源于所有的代数函数、超越函数，都是建立在公理集合论的基础之上的。&lt;/p&gt;

&lt;p&gt;虽然分类常量函数原则上能够构建我们实际所需的所有函数，但用这种最基础的方法，未必见得是最高效的，比如上面提到的对正弦函数的建模，所以有的时候，我们可能会使用线性或者高阶函数来建模。&lt;/p&gt;

&lt;h4 id=&quot;1.2.2-模型复杂度&quot;&gt;1.2.2 模型复杂度&lt;/h4&gt;

&lt;p&gt;更进一步，不但特征工程是对样本进行分类的过程，实际上所有机器学习方法，也就在于找到样本空间中内在的分类结构。模型复杂度可以用模型中蕴含的分类数量来刻画。比如在KNN方法中有个结论，k=全集的时候，模型最简单，k=1时，模型最复杂。有些人可能觉得有点儿违反直觉，也有人觉得这有道理，却难以说出个所以然来。如果我们将样本点的邻域作为一个类别的话，易知邻域小则有效分类数目多，邻域大则有效分类数目小，由于分类数目刻画模型复杂度，因而KNN的模型复杂度与K的关系就显而易见了。&lt;/p&gt;

&lt;h2 id=&quot;2-logistic回归&quot;&gt;2 Logistic回归&lt;/h2&gt;

&lt;p&gt;对于Logistic回归很多文献都是从广义线性回归，或者从高斯分布乃至于一般的指数族分布的角度来描述，虽然这种角度利于导出Logistic回归的表达式。但个人以为这对于指导特征的构造并没有什么用。下面将主要以第一节所叙述的分类的视角来探索特征及其参数的物理意义。&lt;/p&gt;

&lt;p&gt;Logistic回归的形式为
$P(y=1|x)=\sigma \lbrace \sum\limits_{i=0}^{K}\lambda_i x_i \rbrace$
其中:
$\sigma(x)=\frac{1}{1+\exp(-x)}$。&lt;/p&gt;

&lt;h3 id=&quot;2.1-0-1离散特征&quot;&gt;2.1  0-1离散特征&lt;/h3&gt;

&lt;p&gt;由于categorical（分类、枚举）特征，均需转换为0-1离散特征来使用，因此下文中我们不对categorical特征进行单独讨论。&lt;/p&gt;

&lt;p&gt;先看最简单的情况，假设：$K=1$,$x_i \in \lbrace0,1\rbrace$。此时，$\lambda x=\ln \frac{p}{1-p}$,若$x=0$,则$p=1/2$,若$x=1$,则$\lambda=\ln \frac{p}{1-p}$。表明当特征不满足的时候$x=0$，概率为0.5。当特征满足的时候，$\lambda$等于几率的对数。&lt;/p&gt;

&lt;p&gt;其物理意义是，当某种特征满足的时候，我们能够知道结果为正例的概率，如果不满足，则根据最大熵原则，估计其概率为0.5。由此可知Logistic回归与最大熵模型是一回事。再假设定义一种特征使得$x=1$总是成立的，因此全体样例均满足，那么正例在全体样本中的总体概率等于
$\sigma (\lambda)$，这种$\lambda$
对应着(仅当K=1时等于)Logistic回归中的偏置:
$P(y=1|x)=\sigma \lbrace\sum \limits_{i=0}^{K}\lambda_i x_i \rbrace$,
其中
$x_0==1$&lt;/p&gt;

&lt;p&gt;在K&amp;gt;1的情况下，假设前K个$ x_i $的特征值已经给定，其概率为
$p _ {k}$ ,对于第K+1个输入特征$x_ {k+1}$,若值为0，即特征不满足，则$p _ {k+1=p_k}$,概率值不变。当特征满足的时候，$p _ {k+1}$与$p _ k$相比，$\sigma$函数中的线性和部分，仅仅是增加了$\lambda _ {k+1}$，其意义是当我们观察到样例符合某种特征时，为正例的概率在原有的基础之上进行一定调整。若其他特征均为0，那么$\sigma(\lambda_ {k+1})$就等于正例的概率，换句话是说，满足特征$k+1$的样本集合$S_ {k+1}$中减去其他特征所定义的集合,设为
$ \hat{S} _ {k+1}$,平均有$ |\hat{S} _ {k+1}| \sigma(\lambda _ {k+1})$个正例。
&lt;div align=&#39;center&#39;&gt;&lt;img  src=&#39;/media/class-logistic.png&#39; alt=&quot;&quot;&gt;&lt;br&gt;&lt;label&gt;图1：特征分类&lt;/label&gt;&lt;/div&gt;&lt;br/&gt;
在图1中，$x_i,x_j,x_k$三个椭圆分别代表三个特征为1时的分类。当Logistic回归模型中只加入一个特征比如$x_i$时，训练所得的参数为$\lambda_i$，那么图中$x_i$对应的完整椭圆中正例的概率等于$\sigma(\lambda_i)$。而当模型中加入多个特征时，$x_i$训练所得的$\lambda_i$只对应绿色部分所涵盖的正例的概率（百分比）。&lt;/p&gt;

&lt;h3 id=&quot;2.2-特征交互&quot;&gt;2.2 特征交互&lt;/h3&gt;

&lt;p&gt;仅仅考虑$x_i,x_j$两种特征的情况下，此时产生三个特征子集:$\lbrace x_i=1 \land x_j=0 ,x_i=0 \land x_j=1, x_i=1 \land x_j=1 \rbrace$，要表达这些子集下的正例概率需要3个参数。然而在Logistic回归中只有2个参数，因此这只能是一种近似。如果需要更精确的结果，应该构造交叉特征$(x_i,x_j)\in \lbrace (1,0),(0,1),(1,1) \rbrace$。如果两个特征子集没有交集，或者一个是另外一个子集，由于此时只需要2个参数刻画，Logistic回归能够获得最优组合结果，这表明Logistic回归与朴素贝叶斯相比对特征之间的独立性不敏感。&lt;/p&gt;

&lt;p&gt;然而我们仍然希望构造的特征具有足够的差异，因为两个相似特征产生的分类效果主要由交叉部分$\lbrace x_i=1 \land x_j=1\rbrace$决定，换言之，两个相似的特征的组合效果会不比单独特征使用时有较大提升，相似性度量可用Jaccard相似度: $\frac{P(x_i=1,x_j=1)}{P(x_i=1)+P(x_j=1)-P(x_i=1,x_j=1)}$。&lt;/p&gt;

&lt;h3 id=&quot;2.3-连续特征&quot;&gt;2.3 连续特征&lt;/h3&gt;

&lt;p&gt;有些特征的值不止取值{0，1}，而是连续的，连续值可以表明样例对该特征（集合）的隶属度，比如在文物鉴定中，专家可能根据某些特征来判定是否是真伪，但这种特征在文物中可能不易辨析清楚，因此是以某种概率满足该特征。连续值还可以是现实中的一些计量，如在判断一个人是否会加入某个俱乐部的时候，他的资产可能是一个有效的具有连续值的特征。在使用连续值特征的过程中，一个主要困难是估计该特征值的大小如何影响结果分类的对数几率，比如它是否是线性的，如果不是那么要设计什么样的变换函数。根据第一节的讨论，我们知道有一个简单的方法是使用分类常量去拟合任意函数，从而避免这些问题。&lt;/p&gt;

&lt;p&gt;在Logistic回归中，如果有一个连续特征是 $x_i$,参数为$\lambda_i$，其组合为$\lambda_i x_i$，从纯函数的角度进行分析可以发现$\lambda_i x_i$定义了一个线性函数族，该函数族的参量只有一个$\lambda_i$，自由度为1，它能表达的函数空间很有限。如果我们将其离散化为M个0-1特征，那么便有$\lambda_i x_i=\sum\limits_{m=1}^M{ \lambda _{im} c_m}$,其中任意$x_i \in \bigcup_m c_m\ $,易知当M足够大的时候，0-1特征族能近似表达任意函数，而不仅仅是某种函数比如线性、二次、对数。&lt;/p&gt;

&lt;p&gt;离散化方法（分类常量）的使用，将我们从某个连续特征与最终概率之间的函数关系的分析中解脱出来。&lt;/p&gt;

&lt;p&gt;要特别注意的是，如果连续特征过分离散化则会导致特征过多以至于接近训练样例的数目，这种特征会与样例接近形成一一映射，将在实质上记住训练样例的结果从而导致&lt;strong&gt;过拟合&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;2.4-离散特征的连续化-logistic回归作为ensemble-method&quot;&gt;2.4  离散特征的连续化-Logistic回归作为ensemble method&lt;/h3&gt;

&lt;p&gt;使用0-1特征，特别是将连续特征0-1离散化后，特征数目会急剧增大，高维数据下，训练的压力会比较大。这时可以通过将某些0-1离散特征族预先训练（称为基分类器，base classifer），将其预测概率输出经过对数几率变换作为下一级的Logistic回归分类器（称为集成分类器，ensemble classifer）的输入。这相当于将基分类器作为一个特征提取器。&lt;/p&gt;

&lt;p&gt;这是因为0-1特征族$x_{im}$对应着m个$\lambda_{im}$值，因而可等价地定义一个新的属性$x_i=\sum\limits_ {m=1}^{M}\lambda_ {im}x_{im}$，因为在该特征族内各特征是互斥的，当只考虑这个特征族时，此属性值可以统计获得。比如在ijcai2015竞赛预测是否是回头客的baseline算法中，使用4995个merchant_id为特征，这些特征取值为${0,1}$，因此可以统计某个merchnat_id特征满足时label为1的概率。可以看做是属性&amp;quot;merchant_id&amp;quot;的值。如此，我们将4995个0-1离散特征收缩为1个连续特征。经过实验验证，这种方式所得到的结果与使用merchant_id所得到的结果一致，并且能极大地提高计算速度。&lt;/p&gt;

&lt;p&gt;当然这种方法是一种有效的近似，而不是原模型的等价物。因为基分类器在集成分类器中训练所对应的只有一个$\lambda$值，这意味着基分类器的多个特征输入在最后的预测过程中，其分类信号强度被平均了。&lt;/p&gt;

&lt;h2 id=&quot;3-总结&quot;&gt;3  总结&lt;/h2&gt;

&lt;p&gt;在Logistic回归的特征构造过程中，应该优先使用0-1特征，也就是说要致力于寻找这样的样本子集（分类）：该子集中的正例分布要与总体正例分布相差越大越好；同时该子集不能过小，比如只包含1个样本，这样将导致过拟合；这些子集之间应该具备足够的差异，过于相似的特征不能取得很好的组合效果。&lt;/p&gt;

&lt;p&gt;在使用连续特征时，要谨慎地考虑其是否具有线性效应，否则应该将其0-1离散化，或者选择恰当的变换函数。不进行细致处理的连续特征往往不能得到预期的结果。&lt;/p&gt;

&lt;p&gt;最后，对于某些高维互斥的0-1离散化特征族，可以将其聚合成一个连续特征以降低特征维数，提高模型训练的速度。&lt;/p&gt;
</description>
				<pubDate>Mon, 08 Jun 2015 00:00:00 +0800</pubDate>
				<link>http://lihui.github.io/2015/06/logistic</link>
				<guid isPermaLink="true">http://lihui.github.io/2015/06/logistic</guid>
			</item>
		
	</channel>
</rss>
