---
layout: post
title: 逻辑斯蒂回归的特征工程
author: 灵元
---


##1 函数的表示


 

###1.1 人类使用的函数

从中学生开始，我们所学的大多是连续函数，有些函数特别漂亮，比如抛物线，正弦函数。这些函数似乎以一个区区简单的公式，就描述了无限的x与y的对应关系。这些代数函数和超越函数，都是基于代数运算规则来定义的。其中代数函数由有限个代数函数构成，超越函数则通过无限个迭代步以代数函数为基础合成，在这个迭代过程中，仅有有限个参数或者没有参数来控制迭代规则。例如指数函数的泰勒展开式就定义了这样的迭代规则。用这些函数来表达映射关系的时候，对于任意的定义域和值域，均只需要有限的参数就可以确定映射的唯一性。这是一种非常优秀的特性，能通过抓住极少数变化的要素（参数），就可以确定描述大量事实之间的依存关系。当找到了一个这样的函数我们一般认为找到了一个简洁的规律。通过这个规律能够推演出无限的事实来，在自然科学研究中叫规律的普适性。在机器学习中，称为泛化能力。

###1.2 机器使用的函数

机器学习要寻找这种具备很好的泛化能力的模型。但计算机更加擅长处理数值计算问题，对符号（公式、计算规则）很难处理。由于计算公式的多样性与可组合性，机器不可能通过穷尽所有的公式所构成的函数空间，更由于某些函数可能没有闭形式，因此机器学习所学习到的函数一般是限定在某个函数空间下，通过搜索其参数的恰当值而得到的。


这样看起来似乎计算机的处理能力很受限，然而我们可以通过使用分段常量或者分段线性函数来逼近任意足够光滑的函数。如果用一个参数来定义一个分段常量函数，或者两个参数来定义分段线性函数。同时将该参数数量控制在一个足够大的有限量上。通过这种方式计算机在理论上能够建模实际所需的任意函数。当然我们要避免让计算机直接构造每一对映射，定义域一般是巨大的，无穷的，甚至是不可数的。如果只记忆部分样本，无法对不在记忆中的实例进行推断，因此不具备推广（泛化）能力。分段常量或者分段线性函数提供了一种近似预测相邻未知点的值的方法。通过使用比原数据更少的模型参数来表达原始海量的值对。只要模型参数的数量比原数据更少，就能够获得某种程度的泛化能力。实际上能够使用越少的参数（或者同样的参数数目下参数越稀疏）拟合数据就越可能找到了“规律”。

####1.2.1 分类常量
作为一个例子，我们来考虑如何以有限的参数，以分段常量函数来近似具有无限个值对的正弦函数。首先，我们能够以足够多的有限个分段常量函数获得所需足够精度的一个周期的近似，假设该数目为k，周期为T。其次，我们可以针对x的每一个值点构造k个特征，若x处于区间 [nT+iT/k,nT+(i+1)T/k),则令x的第i个特征为1，其余特征为0. 如果x满足满足第i个特征，我们便给出其值为第i个常量函数的值。这样，我们便通过使用一种固定特征提取的方式，和有限个分段常量函数（有限个参数），建模了正弦函数。这个模型具有很好的泛化能力。其精度随着参数数目的增加而增加。当特征数目确定的时候，可以通过平方误差损失函数来训练模型。

由上可知，如何找到特征函数是问题的关键，而参数数量的大小在这里对应着特征的多少，决定了模型的精确度。当然由于正弦函数是周期函数，容易理解可以用有限个分段常量函数来表达，对于有些函数，比如二次函数，如果值域是无穷的，那么便无法通过有限个分段常量函数来近似。然而在实际问题中，我们所面临的情况，都是定义域（自变量）非常巨大，以至于在训练数据中不可能全部出现，但需要预测的值往往不是无穷区间，而是有着一个确定的空间，比如对于典型的二分类问题，只有两个值。对于这类问题，虽然不像正弦函数一样都具有良好的周期性，但如同拟合正弦函数一样，如果找到一些方法，将输入实例进行有效折叠（分类，等价于构造有效特征），那么仍然可以用有限的参数来建模函数。

换个角度来看，每构造一个特征，其实都是给出了一个分类方法。正弦函数构造特征的过程，就是将属于某个分段，以及所有与该分段跨度为T的数均划分为一类，将其对应于某个常量，从这个意义上来说，上述**分段常量**应该称为**分类常量**。

分类常量作为万能建模法，对机器学习在能力上给出了一个保障。若论原初，这个保障来源于所有的基本函数、超越函数，都是建立在公理集合论的基础之上的。

虽然分类常量函数原则上能够构建我们实际所需的所有函数，但用这种最基础的方法，未必见得是最高效的，比如上面提到的对正弦函数的建模，所以有的时候，我们可能会使用线性或者高阶函数来建模。

####1.2.2 模型复杂度
更进一步，不但特征工程是对样本进行分类的过程，实际上所有机器学习方法，也就在于找到样本空间中内在的分类结构。模型复杂度可以用模型中蕴含的分类数量来刻画。比如在KNN方法中有个结论，k=全集的时候，模型最简单，k=1时，模型最复杂。有些人可能觉得有点儿违反直觉，也有人觉得这有道理，确难以说出个所以然来。如果我们将样本点的邻域作为一个类别的话，易知邻域小则有效分类数目就多，邻域大则有效分类数目就小，由于分类数目刻画模型复杂度，因而KNN的模型复杂度与K的关系就显而易见了。





##2 逻辑斯蒂回归

对于逻辑斯蒂回归很多文献都是从广义线性回归，或者从高斯分布乃至于一般的指数族分布的角度来描述，虽然这种角度利于导出逻辑斯蒂回归的表达式。但个人以为这对于指导特征的构造并没有什么卵用。下面将主要以第一节所叙述的分类的视角来探索特征及其参数的物理意义。

逻辑斯蒂回归的形式为
$P(y=1|x)=\sigma \lbrace \sum\limits_{i=0}^{K}\lambda_i x_i \rbrace$
其中:
$\sigma(x)=\frac{1}{1+\exp(-x)}$。

###2.1  0-1离散特征

由于category（分类、枚举）特征，均需转换为0-1离散特征来使用，因此下文中我们不对category特征进行单独讨论。

先看最简单的情况，假设：$K=1$,$x_i \in \{0,1\}$。此时，$\lambda x=\ln \frac{p}{1-p}$,若$x=0$,则$p=1/2$,若$x=1$,则$\lambda=\ln \frac{p}{1-p}$。表明当特征不满足的时候$x=0$，概率为0.5。当特征满足的时候，$\lambda$等于几率的对数。

其物理意义是，当某种特征满足的时候，我们能够知道结果为正例的概率，如果不满足，则根据最大熵原则，估计其概率为0.5。由此可知逻辑斯蒂回归与最大熵模型是一回事。再假设定义一种特征使得$x=1$总是成立的，因此全体样例均满足，那么正例在全体样本中的总体概率等于
$\sigma (\lambda)$，这种$\lambda$
对应着逻辑斯蒂回归中的偏置:
$P(y=1|x)=\sigma \lbrace\sum \limits_{i=0}^{K}\lambda_i x_i \rbrace$,
其中
$x_0==1$


在K>1的情况下，假设前K个$ x_i $的特征值已经给定，其概率为
$p _ {k}$ ,对于第K+1个输入特征$x_ {k+1}$,若值为0，即特征不满足，则$p _ {k+1=p_k}$,概率值不变。当特征满足的时候，$p _ {k+1}$与$p _ k$相比，$\sigma$函数中的线性和部分，仅仅是增加了$\lambda _ {k+1}$，其意义是当我们观察到样例符合某种特征时，为正例的概率在原有的基础之上进行一定调整。若其他特征均为0，那么$\sigma(\lambda_ {k+1})$就等于正例的概率，换句话是说，满足特征$k+1$的样本集合$S_ {k+1}$中减去其他特征所定义的集合,设为
$ \hat{S} _ {k+1}$,平均有$ |\hat{S} _ {k+1}| \sigma(\lambda _ {k+1})$个正例。
<div align='center'><img  src='/media/class-logistic.png' alt=""><br><label>图1：特征分类</label></div><br/>
在图1中，$x_i,x_j,x_k$三个椭圆分别代表三个特征为1时的分类。当逻辑斯蒂回归模型中只加入一个特征比如$x_i$时，训练所得的参数为$\lambda_i$，那么图中$x_i$对应的完整椭圆中正例的概率等于$\sigma(\lambda_i)$。而当模型中加入多个特征时，$x_i$训练所得的$\lambda_i$只对应绿色部分所涵盖的正例的概率（百分比）。

###2.2 特征交互
仅仅考虑$x_i,x_j$两种特征的情况下，此时产生三个特征子集:$\lbrace x_i=1 \land x_j=0 ,x_i=0 \land x_j=1, x_i=1 \land x_j=1 \rbrace$，要表达这些子集下的正例概率需要3个参数。然而在逻辑回归中只有2个参数，因此这只能是一种近似。如果需要更精确的结果，应该构造交叉特征$(x_i,x_j)\in \lbrace (1,0),(0,1),(1,1) \rbrace$。如果两个特征子集没有交集，或者一个是另外一个子集，由于此时只需要2个参数刻画，逻辑斯蒂回归能够获得最优组合结果，这表明逻辑斯蒂回归与朴素贝叶斯相比对特征之间的独立性不敏感。

然而我们仍然希望构造的特征具有足够的差异，因为两个相似特征产生的分类效果主要由交叉部分$\lbrace x_i=1 \land x_j=1\rbrace$决定，换言之，两个相似的特征的组合效果会不比单独特征使用时有较大提升，相似性度量可用Jaccard相似度: $\frac{P(x_i=1,x_j=1)}{P(x_i=1)+P(x_j=1)-P(x_i=1,x_j=1)}$。

###2.3 连续特征


有些特征的值不止取值\{0，1\}，而是连续的，连续值可以表明样例对该特征（集合）的隶属度，比如在文物鉴定中，专家可能根据某些特征来判定是否是真伪，但这种特征在文物中可能不易辨析清楚，因此特征本身的满足与否本身也存在一定的概率。连续值还可以是现实中的一些计量，如在判断一个人是否会加入某个俱乐部的时候，他的资产可能是一个有效的具有连续值的特征。在使用连续值特征的过程中，一个主要困难是估计该特征值的大小，如何影响预测结果的值，比如它是否是线性的，如果不是那么要设计什么样的变换函数。根据第一节的讨论，我们知道有一个简单的方法是使用分类常量去拟合任意函数，从而避免这些问题。

在逻辑斯蒂回归中，如果有一个连续特征是 $x_i$,参数为$\lambda_i$，其组合为$\lambda_i x_i$，从纯函数的角度进行分析可以发现$\lambda_i x_i$定义了一个线性函数族，该函数族的参量只有一个$\lambda_i$，自由度为1，它能表达的函数空间很有限。如果我们将其离散化为M个0-1特征，那么便有$\lambda_i x_i=\sum\limits_{m=1}^M{ \lambda _{im} c_m}$,其中任意$x_i \in \bigcup_m c_m\ $,易知当M足够大的时候，0-1特征族能近似表达任意函数，而不仅仅是某种函数比如线性、二次、对数。

离散化方法（分类常量）的使用，将我们从某个连续特征与最终概率之间的函数关系的分析中解脱出来。


要特别注意的是，如果连续特征过分离散化则会导致特征过多以至于接近训练样例的数目，这种特征会与样例接近形成一一映射，将在实质上记住训练样例的结果从而导致**过拟合**。


###2.4  离散特征的连续化-逻辑斯蒂回归作为ensemble method

使用0-1特征，特别是将连续特征0-1离散化后，特征数目会急剧增大，高维数据下，训练的压力会比较大。这时可以通过将某些0-1离散特征族预先训练（称为基分类器，base classifer），将其预测概率输出经过对数几率变换作为下一级的逻辑斯蒂回归分类器（称为集成分类器，ensemble classifer）的输入。这相当于将基分类器作为一个特征提取器。

这是因为0-1特征族$x_{im}$对应着m个$\lambda_{im}$值，因而可等价地定义一个新的属性$x_i=\sum\limits_ {m=1}^{M}\lambda_ {im}x_{im}$，因为在该特征族内各特征是互斥的，当只考虑这个特征族时，此属性值可以统计获得。比如在ijcai2015竞赛预测是否是回头客的baseline算法中，使用4995个merchant\_id为特征，这些特征取值为$\{0,1\}$，因此可以统计某个merchnat\_id特征满足时label为1的概率。可以看做是属性"merchant\_id"的值。如此，我们将4995个0-1离散特征收缩为1个连续特征。经过实验验证，这种方式所得到的结果与使用merchant\_id所得到的结果一致，并且能极大地提高计算速度。

当然这种方法是一种有效的近似，而不是原模型的等价物。因为基分类器在集成分类器中训练所对应的只有一个$\lambda$值，这意味着基分类器的多个特征输入在最后的预测过程中，其分类信号强度被平均了。


##3  总结


在逻辑斯蒂回归的特征构造过程中，应该优先使用0-1特征，也就是说要致力于寻找这样的样本子集（分类）：该子集中的正例分布要与总体正例分布相差越大越好；同时该子集不能过小，比如只包含1个样本，这样将导致过拟合；这些子集之间应该具备足够的差异，过于相似的特征不能取得很好的组合效果。

在使用连续特征时，要谨慎地考虑其是否具有线性效应，否则应该将其0-1离散化，或者选择恰当的变换函数。不进行细致处理的连续特征往往不能得到预期的结果。

最后，对于某些高维互斥的0-1离散化特征族，可以将其聚合成一个连续特征以降低特征维数，提高模型训练的速度。

